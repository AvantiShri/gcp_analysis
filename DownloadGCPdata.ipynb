{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvantiShri/gcp_analysis/blob/main/DownloadGCPdata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsKNiNe8VscQ",
        "outputId": "3438e660-a93a-48a2-9fea-ae70bc5b99c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_D_gJOzgPxXO",
        "outputId": "5e5264ad-7d79-4b57-84c3-36c18136eeb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/GCP_data\n",
            "--2023-10-26 01:29:09--  https://global-mind.org/pred_formal.html\n",
            "Resolving global-mind.org (global-mind.org)... 162.245.217.130\n",
            "Connecting to global-mind.org (global-mind.org)|162.245.217.130|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘pred_formal.html’\n",
            "\n",
            "pred_formal.html        [ <=>                ] 184.64K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-10-26 01:29:09 (2.86 MB/s) - ‘pred_formal.html’ saved [189067]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#extract the table from https://global-mind.org/pred_formal.html\n",
        "#(the \"create a text file for download\" link does not appear to be working)\n",
        "\n",
        "%cd /content/drive/MyDrive/GCP_data\n",
        "!wget https://global-mind.org/pred_formal.html -O pred_formal.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGsYRrxCYOea",
        "outputId": "6e2955c1-d1dc-4f07-da21-1a8ce8fd984a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/GCP_data\n",
            "<!DOCTYPE html>\n",
            "<html lang=\"en-US\"><head>\n",
            "<!-- SSI pageheaders content -->\r\n",
            "<meta charset=\"utf-8\" />\r\n",
            "<meta http-equiv=\"cleartype\" content=\"on\" />\r\n",
            "<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />\r\n",
            "<meta name=\"author\" content=\"Roger Nelson\" />\r\n",
            "<meta name=\"keywords\" content=\"consciousness,group consciousness,Global Consciousness Project,Roger Nelson,GCP,resonance,global consciousness,synchronized consciousness,mind,world,global,gaia,anomalies,parapsychology,psi,random event,REG,RNG,subtle energy,millennium\" />\r\n",
            "<meta http-equiv=\"imagetoolbar\" content=\"no\" />\r\n",
            "<meta name=\"Classification\" content=\"science\" />\r\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/GCP_data\n",
        "#sanity check\n",
        "!head pred_formal.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PO7tc1E-RRMO"
      },
      "outputs": [],
      "source": [
        "#parse the html\n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(open(\"/content/drive/MyDrive/GCP_data/pred_formal.html\").read())\n",
        "\n",
        "#the 'recipe' and 'statistic' columns don't always get parsed correctly due\n",
        "# to the break, but we can work around it\n",
        "rows = [[td.contents[0].rstrip() for td in row.find_all(\"td\") ]\n",
        "        for row in soup.body.table.tbody.find_all(\"tr\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdY01GlzV4ry"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from datetime import datetime, timedelta\n",
        "import pandas\n",
        "import numpy as np\n",
        "import json'\n",
        "\n",
        "#change to a directory to download the data into\n",
        "%cd /content/drive/MyDrive/GCP_data/raw\n",
        "\n",
        "#populate days_downloaded based on the files that are currently present in\n",
        "# the directory\n",
        "days_downloaded = set([x[11:21] for x in glob.glob(\"basket*.csv\")])\n",
        "\n",
        "exclude_events = [\"283\", #this is New Year Mean 2009 from 2008-12-31 to 2009-01-01,\n",
        "                        # being excluded because data for 2008-12-29 (needed for control 3)\n",
        "                        # is absent: https://global-mind.org/data/eggsummary/2008/\n",
        "                  \"387\", # Libya Day 1 Rebels Take Tripoli, 2011-08-21; similar situation - data seems to be missing/empty: https://global-mind.org/data/eggsummary/2011/\n",
        "                  \"388\", #Event on 2011-08-23, similar problem\n",
        "                  \"392\", #the 'end' in the table is before the 'start' so I'm guessing there was a mistake and just skipping the event\n",
        "                  \"469\", #Event on 11/05 which had only 54000 rows of data for the day\n",
        "                  \"470\", #Event on 11/07 which includes the 11/05 data in the control timeperiods\n",
        "                  ]\n",
        "\n",
        "pick_up_from = 0 #in case execution paused and you need to pick up from this event onwards\n",
        "force_rerun = [] #events to rerun the download for\n",
        "\n",
        "included_events = 0\n",
        "\n",
        "#fetch data for all the relevant days. We will stick them together later\n",
        "for row in rows:\n",
        "  if row[-1].startswith(\"Yes\") and row[-3].startswith(\"Stouffer Z\"): #only keep those rows that were used in analysis and used Stouffer Z\n",
        "    start, end = row[2], row[3]\n",
        "    start_datetime = datetime.strptime(start, '%Y-%m-%d %H:%M:%S')\n",
        "    end_datetime = datetime.strptime(end, '%Y-%m-%d %H:%M:%S')\n",
        "    duration = 1 + (end_datetime - start_datetime).total_seconds() #in seconds\n",
        "\n",
        "    print(\"\\n------------------------------------\")\n",
        "    print(row[0], row[1], row[2], row[3]) #eventnum name start end\n",
        "    print(\"Duration in seconds:\", duration)\n",
        "    print(\"------------------------------------\")\n",
        "\n",
        "    event_num = row[0]\n",
        "\n",
        "    #exclude events longer than 1.5days long, and also events that are in the excluded category\n",
        "    if ((duration <= 36*60*60 and event_num not in exclude_events)):\n",
        "      control1_datetime_start = start_datetime - timedelta(seconds=duration)\n",
        "      control1_datetime_end = control1_datetime_start + timedelta(seconds=duration-1) #inclusive end\n",
        "      control2_datetime_start = end_datetime + timedelta(seconds=1)\n",
        "      control2_datetime_end = control2_datetime_start + timedelta(seconds=duration-1)\n",
        "\n",
        "      control3_datetime_start = control1_datetime_start - timedelta(seconds=duration)\n",
        "      control3_datetime_end = control3_datetime_start + timedelta(seconds=duration-1)\n",
        "      control4_datetime_start = control2_datetime_end + timedelta(seconds=1)\n",
        "      control4_datetime_end = control4_datetime_start + timedelta(seconds=duration-1)\n",
        "\n",
        "      control5_datetime_start = control3_datetime_start - timedelta(seconds=duration)\n",
        "      control5_datetime_end = control5_datetime_start + timedelta(seconds=duration-1)\n",
        "      control6_datetime_start = control4_datetime_end + timedelta(seconds=1)\n",
        "      control6_datetime_end = control6_datetime_start + timedelta(seconds=duration-1)\n",
        "\n",
        "      print(\"Control1 start and end:\", control1_datetime_start, control1_datetime_end)\n",
        "      print(\"Control2 start and end:\", control2_datetime_start, control2_datetime_end)\n",
        "      print(\"Control3 start and end:\", control3_datetime_start, control3_datetime_end)\n",
        "      print(\"Control4 start and end:\", control4_datetime_start, control4_datetime_end)\n",
        "      print(\"Control5 start and end:\", control5_datetime_start, control5_datetime_end)\n",
        "      print(\"Control6 start and end:\", control6_datetime_start, control6_datetime_end)\n",
        "\n",
        "      included_events += 1\n",
        "\n",
        "      #get all the days between control5 start and control6 end\n",
        "      days = []\n",
        "      start_day = datetime.strptime(control5_datetime_start.strftime('%Y-%m-%d'), '%Y-%m-%d')\n",
        "      end_day = datetime.strptime(control6_datetime_end.strftime('%Y-%m-%d'), '%Y-%m-%d')\n",
        "      day_to_add = start_day\n",
        "      while True:\n",
        "        if (day_to_add not in days):\n",
        "          days.append(day_to_add.strftime('%Y-%m-%d'))\n",
        "        if (day_to_add == end_day):\n",
        "          break\n",
        "        day_to_add = day_to_add + timedelta(days=1)\n",
        "      print(\"Days:\", days)\n",
        "\n",
        "      #now download the relevant data\n",
        "      for day in days:\n",
        "        if (day not in days_downloaded) or (event_num in force_rerun):\n",
        "          year = day[:4]\n",
        "          filename = \"basketdata-\"+day\n",
        "          !wget -r -nH --cut-dirs=3 --limit-rate=125k https://global-mind.org/data/eggsummary/{year}/{filename}.csv.gz\n",
        "          !gunzip -f {filename}.csv.gz\n",
        "          print(\"\\nROWS:\")\n",
        "          !wc -l {filename}.csv\n",
        "          days_downloaded.add(day)\n",
        "\n",
        "      if int(event_num) >= pick_up_from:\n",
        "        event_metadata = {}\n",
        "        for spanname, startdt, enddt in [('test', start_datetime, end_datetime),\n",
        "                                        ('control1', control1_datetime_start, control1_datetime_end),\n",
        "                                        ('control2', control2_datetime_start, control2_datetime_end),\n",
        "                                        ('control3', control3_datetime_start, control3_datetime_end),\n",
        "                                        ('control4', control4_datetime_start, control4_datetime_end),\n",
        "                                        ('control5', control5_datetime_start, control5_datetime_end),\n",
        "                                        ('control6', control6_datetime_start, control6_datetime_end)]:\n",
        "          numpy_outfile = '/content/drive/MyDrive/GCP_data/extracted/Event'+event_num+\"_\"+spanname+\"_eggvalues.npy\"\n",
        "          if (os.path.exists(numpy_outfile)==False):\n",
        "            #get all the days between control5 start and control6 end\n",
        "            days = []\n",
        "            start_day = datetime.strptime(startdt.strftime('%Y-%m-%d'), '%Y-%m-%d')\n",
        "            end_day = datetime.strptime(enddt.strftime('%Y-%m-%d'), '%Y-%m-%d')\n",
        "            day_to_add = start_day\n",
        "            while True:\n",
        "              if (day_to_add not in days):\n",
        "                days.append(day_to_add.strftime('%Y-%m-%d'))\n",
        "              if (day_to_add == end_day):\n",
        "                break\n",
        "              day_to_add = day_to_add + timedelta(days=1)\n",
        "            print(\"Days:\", days)\n",
        "            #Now extract the data with pandas\n",
        "            df = pandas.concat([pandas.read_csv(\"basketdata-\"+day+\".csv\", skiprows=8)\n",
        "                                          for day in days])\n",
        "            print(\"Concatenated df has\",len(df),\"rows\")\n",
        "            filtered_rows = df[((df['gmtime'] >= startdt.timestamp())\n",
        "                                & (df['gmtime'] <= enddt.timestamp()))]\n",
        "            print(\"Num filtered rows in \"+spanname+\": \", len(filtered_rows))\n",
        "            egg_values = filtered_rows.iloc[:, 3:]\n",
        "            np.save(numpy_outfile, egg_values)\n",
        "            num_rows = len(filtered_rows)\n",
        "          else:\n",
        "            num_rows = len(np.load(numpy_outfile))\n",
        "          event_metadata[spanname] = {'start_timestamp':startdt.timestamp(),\n",
        "                                      'end_timestamp':enddt.timestamp(),\n",
        "                                      \"num_rows\":num_rows}\n",
        "        #print out the metadata at the end\n",
        "        with open('/content/drive/MyDrive/GCP_data/extracted/Event'+event_num+\"_metadata.json\", \"w\") as f:\n",
        "          f.write(json.dumps(event_metadata, indent=4))\n",
        "        #sanity check all spans have the same length\n",
        "        assert len(set([x['num_rows'] for x in event_metadata.values()]))==1, event_metadata\n",
        "    else:\n",
        "      print(\"SKIPPING:\", event_num)\n",
        "\n",
        "print(\"Events included:\", included_events)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yDoNkJmMn06z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1af7dc8f-c613-4a1c-ea6f-8ff4ab3a7d19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "430"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "included_events"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Miv9JMijnYkY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPcZ6sAO8Zo5P+N96fP5ylf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}